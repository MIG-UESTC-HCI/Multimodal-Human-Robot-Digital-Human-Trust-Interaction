# Review-and-Outlook-of-Shared-Multi-Modal-Trustworthy-Human-Machine-Interaction-Research[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/XingfuCao/Review-and-Outlook-of-Shared-Multi-Modal-Trustworthy-Human-Machine-Interaction-Research)

A curated list of research papers in human-machine interaction.

## ðŸ’¬ News
**[2023/04/2]**: Create this repository.

<!-- 1. First Author. **Paper Name**. Conf. [[Paper]]() [[Code]]() [[Website]]() -->

## multimodal human-robot interaction<br>
#### 2021
1. Yixin Chen, Qing Li, et al. **YouRefIt: Embodied Reference Understanding with Language and Gesture**. 	ICCV 2021.[[Paper]](https://arxiv.org/abs/2109.03413) 
2. Paul Pu Liang, Yiwei Lyu, et al. **MultiBench: Multiscale Benchmarks for Multimodal Representation Learning**. NeurIPS 2021.[[Paper]](https://arxiv.org/abs/2107.07502) [[Code1]](https://github.com/pliang279/MultiBench) [[Code2]](https://github.com/pliang279/awesome-multimodal-ml)
#### 2022
1. Tim Salzmann, Marco Pavone, et al. **Motron: Multimodal Probabilistic Human Motion Forecasting**. 	CVPR 2022. [[Paper]](https://arxiv.org/abs/2203.04132) [[Code]](https://github.com/TUM-AAS/motron-cvpr22)
2. Robert L. Wilson, Daniel Browne, et al. **A Virtual Reality Simulation Pipeline for Online Mental Workload Modeling**. VR 2022.[[Paper]](https://arxiv.org/abs/2111.03977) 
#### 2023
1. Eley Ng, Ziang Liu, et al. **It Takes Two: Learning to Plan for Human-Robot Cooperative Carrying**. ICRA 2023. [[Paper]](https://arxiv.org/abs/2209.12890) [[Code1]](https://github.com/eleyng/table-carrying-ai)[[Code2]](https://github.com/eleyng/cooperative_planner)

## multimodal fusion<br>
#### 2020
1. Shaofei Huang, Tianrui Hui, et al.**Referring Image Segmentation via Cross-Modal Progressive Comprehension**. CVPR 2020. [[Paper]](https://arxiv.org/abs/2010.00514) [[Code]](https://github.com/spyflying/CMPC-Refseg)
2. Sijie Mai, Haifeng Hu, et al. **Modality to Modality Translation: An Adversarial Representation Learning and Graph Fusion Network for Multimodal Fusion**. AAAI 2020. [[Paper]](https://arxiv.org/abs/1911.07848) [[Code]](https://github.com/TmacMai/ARGF_multimodal_fusion)
#### 2021
1. Kranti Kumar Parida, Siddharth Srivastava, et al. **Beyond Image to Depth: Improving Depth Prediction using Echoes**. CVPR 2021. [[Paper]](https://arxiv.org/abs/2103.08468) [[Code]](https://github.com/krantiparida/beyond-image-to-depth)
2. Yapeng Tian, Chenliang Xu.**Can audio-visual integration strengthen robustness under multimodal attacks?**. 	CVPR 2021. [[Paper]](https://arxiv.org/abs/2104.02000) [[Code]](https://github.com/YapengTian/AV-Robustness-CVPR21)
#### 2022
1. Tim Salzmann, Marco Pavone, . **Motron: Multimodal Probabilistic Human Motion Forecasting**. CVPR 2022. [[Paper]](https://arxiv.org/abs/2203.04132) [[Code]](https://github.com/TUM-AAS/motron-cvpr22) 
2. Matthew Walmer, Karan Sikka, et al. **Dual-Key Multimodal Backdoors for Visual Question Answering**. CVPR 2022. [[Paper]](https://arxiv.org/abs/2112.07668) [[Code]](https://github.com/SRI-CSL/TrinityMultimodalTrojAI)
3. Yikai Wang, Xinghao Chen, et al. **Multimodal Token Fusion for Vision Transformers**. 	CVPR 2022. [[Paper]](https://arxiv.org/abs/2204.08721) [[Code1]](https://github.com/yikaiw/TokenFusion)[[Code2]](https://github.com/huawei-noah/noah-research/tree/master/TokenFusion)[[Code3]](https://github.com/mindspore-ai/models/tree/master/research/cv/TokenFusion)
4. Yihang Yin, Siyu Huang, et al. **BM-NAS: Bilevel Multimodal Neural Architecture Search**. AAAI 2022. [[Paper]](https://arxiv.org/abs/2104.09379) [[Code]](https://github.com/Somedaywilldo/BM-NAS)
5. Hanlei Zhang, Hua Xu, et al. **MIntRec: A New Dataset for Multimodal Intent Recognition**. MM 2022. [[Paper]](https://arxiv.org/abs/2209.04355) [[Code]](https://github.com/thuiar/mintrec)
6. Yikai Wang, Fuchun Sun, et al. **Channel Exchanging Networks for Multimodal and Multitask Dense Image Prediction**. TPAMI 2022. [[Paper]](https://arxiv.org/abs/2112.02252) [[Code]](https://github.com/yikaiw/CEN)
#### 2023
1. Yue Wang, Jinlong Peng, et al. **Multimodal Industrial Anomaly Detection via Hybrid Fusion**. CVPR 2023. [[Paper]](https://arxiv.org/abs/2303.00601) [[Code]](https://github.com/nomewang/m3dm)
2. Jiaming Zhang, Ruiping Liu, et al. **Delivering Arbitrary-Modal Semantic Segmentation**. CVPR 2023. [[Paper]](https://arxiv.org/abs/2303.01480) [[Code]](https://github.com/jamycheung/DELIVER)


## shared awareness cues / shared awareness interaction / AR-based human-robot interaction <br>
#### 2021
1. Huidong Bai, Prasanth Sasikumar, et al. **A User Study on Mixed Reality Remote Collaboration with Eye Gaze and Hand Gesture Sharing**. CHI 2021. [[Paper]](https://dl.acm.org/doi/abs/10.1145/3313831.3376550)
#### 2022
1. Diego Paez-Granados, Yujie He, et al. **Pedestrian-Robot Interactions on Autonomous Crowd Navigation: Reactive Control Methods and Evaluation Metrics**. IROS 2022. [[Paper]](https://arxiv.org/abs/2208.02121) [[Code]](https://github.com/epfl-lasa/crowdbot-evaluation-tools)
2. Guang Yang, Juan Cao, et al. **DRAG: Dynamic Region-Aware GCN for Privacy-Leaking Image Detection**. AAAI 2022. [[Paper]](https://arxiv.org/abs/2203.09121) [[Code]](https://github.com/guang-yanng/drag)
3.  Ryo Suzuki, Adnan Karim, et al. **Augmented Reality and Robotics: A Survey and Taxonomy for AR-enhanced Human-Robot Interaction and Robotic Interfaces**. 	CHI 2022. [[Paper]](https://arxiv.org/abs/2203.03254)
#### 2023
1. Theodora Kontogianni, Ekin Celikkan, et al. **Interactive Object Segmentation in 3D Point Clouds**. ICRA 2023. [[Paper]](https://arxiv.org/abs/2204.07183)
2. Ryan K. Cosner, Yuxiao Chen, et al. **Learning Responsibility Allocations for Safe Human-Robot Interaction with Applications to Autonomous Driving**. ICRA 2023. [[Paper]](https://arxiv.org/abs/2303.03504) [[Code]](https://github.com/rkcosner/learning_responsibility_allocation)
3. Catarina G. Fidalgo, MaurÃ­cio Sousa, et al. **MAGIC: Manipulating Avatars and Gestures to Improve Remote Collaboration**. VR 2023. [[Paper]](https://arxiv.org/abs/2302.07909) 
4. Yuchen Cui, Siddharth Karamcheti, et al . **"No, to the Right" -- Online Language Corrections for Robotic Manipulation via Shared Autonomy**. HRI 2023. [[Paper]](https://arxiv.org/abs/2301.02555) [[Code]](https://github.com/stanford-iliad/lilac)

## human-robot trust interaction<br>
#### 2020
1. Zahra Rezaei Khavas, Reza Ahmadzadeh, et al. **Modeling Trust in Human-Robot Interaction: A Survey**. ICSR 2020. [[Paper]](https://arxiv.org/abs/2011.04796)
2. Tom Weber, Stefan Wermter. **Integrating Intrinsic and Extrinsic Explainability: The Relevance of Understanding Neural Networks for Human-Robot Interaction**.  AAAI 2020. [[Paper]](https://arxiv.org/abs/2010.04602)
#### 2022
1. Maia Stiber, Russell Taylor, et al. **Modeling Human Response to Robot Errors for Timely Error Detection**. IROS 2022. [[Paper]](https://arxiv.org/abs/2208.00565) 
2. Takane Ueno, Yuto Sawa, et al. **Trust in Human-AI Interaction: Scoping Out Models, Measures, and Methods**. CHI 2022. [[Paper]](https://arxiv.org/abs/2205.00189)
3. Brian Tang, Dakota Sullivan, et al. **CONFIDANT: A Privacy Controller for Social Robots**. HRI 2022. [[Paper]](https://arxiv.org/abs/2201.02712)

## human-robot-avatar interaction (in the Metaverse)<br>
#### 2021
1. Max Schwarz, Christian Lenz, et al. **NimbRo Avatar: Interactive Immersive Telepresence with Force-Feedback Telemanipulation**. IROS 2021. [[Paper]](https://arxiv.org/abs/2109.13772)
#### 2022
1.  Chengli Xiao, Ya Fan, et al. **People Do not Automatically Take the Level-1 Visual Perspective of Humanoid Robot Avatars**. IJSR 2022. [[Paper]](https://link.springer.com/article/10.1007/s12369-021-00773-x)
#### 2023
1. Purva Tendulkar, DÃ­dac SurÃ­s, et al. **FLEX: Full-Body Grasping Without Full-Body Grasps**. CVPR 2023. [[Paper]](https://arxiv.org/abs/2211.11903)
